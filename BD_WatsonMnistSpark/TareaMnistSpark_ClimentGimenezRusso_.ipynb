{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarea Spark-MLlib "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Guillermo Climent, Rubén Giménez, Mayra Russo*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enunciado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La tarea propuesta consiste en cargar, procesar y evaluar distintos algoritmos de clustering y clasificación de los que provee la librería de MLlib de Spark sobre el clásico conjunto de datos MNIST.\n",
    "\n",
    "Disponéis de información detallada del dataset, así como resultados de algoritmos empleados para su resolución (en términos de test error rate) en http://yann.lecun.com/exdb/mnist/.\n",
    "\n",
    "En la carpeta de ‘Material’ del ‘Tema 6 – Spark MLlib’ tenéis una sub-carpeta llamada ‘mnist’ donde están los ficheros de entrenamiento y test en formato .csv para su fácil lectura mediante Spark.\n",
    "\n",
    "Se deben probar y comparar varios algoritmos de clasificación aplicando y sin aplicar previamente algoritmos de selección de características (por ejemplo, PCA).\n",
    "\n",
    "Como algoritmos de clasificación se deben compara al menos LogisticRegression y MultiLayerPerceptronClassifier. Para cada método se deberán buscar y optimizar sus parámetros para obtener la mejor clasificación posible.\n",
    "\n",
    "Respecto al clustering, se realizará una prueba con al menos uno de los métodos de Spark para evaluar si uno de estos algoritmos no supervisados es capaz de obtener resultados comparables a los algoritmos supervisados.\n",
    "\n",
    "Se valorará:\n",
    "\n",
    "El uso de Pipelines en el proceso de optimización, entrenamiento y test.\n",
    "\n",
    "Comparar con otros métodos de clasificación además de los dos requeridos.\n",
    "\n",
    "La tarea puede hacerse en parejas o individualmente. Se podrán realizar los programas en R (sparkR) o Python (py-spark). Se recomienda utilizar notebooks para la realización y presentación del trabajo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for a Spark session to start...\n",
      "Spark Initialization Done! ApplicationId = app-20190506091707-0000\n",
      "KERNEL_ID = a82e690b-3659-4c1a-ba08-42fc42e86c1e\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://jkg-deployment-a82e690b-3659-4c1a-ba08-42fc42e86c1e-79ddbbj9vd9:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=spark://jkg-deployment-a82e690b-3659-4c1a-ba08-42fc42e86c1e-79ddbbj9vd9:7077 appName=pyspark-shell>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext, SparkSession\n",
    "\n",
    "# Getting the SparkContext\n",
    "sc = SparkContext()\n",
    "\n",
    "# Initializing the SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "# Initializing Spark Session\n",
    "spark = SparkSession.builder.appName('recommender-system').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1. Load train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import ibmos2spark\n",
    "# @hidden_cell\n",
    "credentials = {\n",
    "    'endpoint': 'https://s3.eu-geo.objectstorage.service.networklayer.com',\n",
    "    'service_id': 'iam-ServiceId-a4631986-b796-40bd-b93a-06065e91801b',\n",
    "    'iam_service_endpoint': 'https://iam.bluemix.net/oidc/token',\n",
    "    'api_key': 'gl2jpib2U2AtMKgyuZX4nk2YLlPFz3DxfM4REYvALTQl'\n",
    "}\n",
    "\n",
    "configuration_name = 'os_068e48156a3944328f96d36308ac1cdf_configs'\n",
    "cos = ibmos2spark.CloudObjectStorage(sc, credentials, configuration_name, 'bluemix_cos')\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "# Please read the documentation of PySpark to learn more about the possibilities to load data files.\n",
    "# PySpark documentation: https://spark.apache.org/docs/2.0.1/api/python/pyspark.sql.html#pyspark.sql.SparkSession\n",
    "# The SparkSession object is already initialized for you.\n",
    "# The following variable contains the path to your file on your IBM Cloud Object Storage.\n",
    "train_file = cos.url('mnist_train.csv.bz2', 'filtradocollab-donotdelete-pr-dchkxqx3kk4b5a')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+---+---+---+---+---+---+\n",
      "|label|1x1|1x2|1x3|1x4|1x5|1x6|1x7|\n",
      "+-----+---+---+---+---+---+---+---+\n",
      "|    5|  0|  0|  0|  0|  0|  0|  0|\n",
      "|    0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|    4|  0|  0|  0|  0|  0|  0|  0|\n",
      "|    1|  0|  0|  0|  0|  0|  0|  0|\n",
      "+-----+---+---+---+---+---+---+---+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#train_file = 'mnist/mnist_train.csv.bz2'\n",
    "train_df_raw = spark.read.csv(train_file, header=True, inferSchema=False)\n",
    "train_df_raw.select(train_df_raw.columns[:8]).show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2. Update data schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "mapping = {column: 'integer' for column in train_df_raw.columns}\n",
    "\n",
    "mapping_dict = dict(mapping)\n",
    "\n",
    "exprs = [col(c).cast(mapping[c]) if c in mapping_dict else c \n",
    "         for c in train_df_raw.columns]\n",
    "\n",
    "train_df_raw = train_df_raw.select(*exprs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3. Add 'features' column as a Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|    5|(784,[152,153,154...|\n",
      "|    0|(784,[127,128,129...|\n",
      "|    4|(784,[160,161,162...|\n",
      "|    1|(784,[158,159,160...|\n",
      "+-----+--------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(inputCols=train_df_raw.columns[1:], outputCol='features')\n",
    "train_df = assembler.transform(train_df_raw)\n",
    "\n",
    "train_df.select(['label', 'features']).show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4. Scaling: create the scaler model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "scaler = StandardScaler(\n",
    "    inputCol='features',\n",
    "    outputCol='scaledFeatures',\n",
    "    withStd=True,\n",
    "    withMean=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1. Define estimators and their hyperparameters\n",
    "1) Logistic Regression <br>\n",
    "2) Random Forest <br>\n",
    "3) Multilayer Perceptron <br>\n",
    "4) K-Means<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression, MultilayerPerceptronClassifier, RandomForestClassifier\n",
    "from pyspark.ml.clustering import KMeans, GaussianMixture\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, ClusteringEvaluator\n",
    "\n",
    "\n",
    "# Extract number of classes\n",
    "classes = train_df.select('label').distinct()\n",
    "n_classes = train_df.select('label').distinct().count()\n",
    "\n",
    "\n",
    "# Extract number of features\n",
    "regex = re.compile(r'[0-9]+x[0-9]+')\n",
    "feature_cols = list(filter(regex.search, train_df.columns))\n",
    "n_features = len(feature_cols)\n",
    "\n",
    "\n",
    "# Define estimators\n",
    "log_reg = LogisticRegression(standardization=False)\n",
    "\n",
    "rf = RandomForestClassifier(labelCol=\"label\", seed = SEED)\n",
    "\n",
    "mlp_layers = [n_features, int(n_features / 2), int(n_features / 4), n_classes]\n",
    "mlp = MultilayerPerceptronClassifier(layers=mlp_layers, blockSize=128, seed=SEED)\n",
    "\n",
    "kmeans = KMeans(k=n_classes, seed=SEED)\n",
    "\n",
    "# Create a list with each defined estimator and its grid param\n",
    "estimator_list = [\n",
    "    (\n",
    "        log_reg, \n",
    "        ParamGridBuilder() \\\n",
    "            .addGrid(log_reg.maxIter, [50, 100]) \\\n",
    "            .addGrid(log_reg.tol, [1E-6]) \\\n",
    "            .addGrid(log_reg.fitIntercept, [False]) \\\n",
    "            .addGrid(scaler.withMean, [False]) \\\n",
    "            .build()\n",
    "    ),\n",
    "    (\n",
    "        rf,\n",
    "        ParamGridBuilder() \\\n",
    "            .addGrid(rf.maxDepth, [10, 15]) \\\n",
    "            .addGrid(rf.maxBins, [5, 10, 20]) \\\n",
    "            .addGrid(rf.numTrees, [10, 20]) \n",
    "            .addGrid(scaler.withMean, [False, True]) \\\n",
    "            .build()\n",
    "    ),\n",
    "    (\n",
    "        mlp, \n",
    "        ParamGridBuilder() \\\n",
    "            .addGrid(mlp.maxIter, [25, 50]) \\\n",
    "            .addGrid(mlp.tol, [1E-6]) \\\n",
    "            .addGrid(scaler.withMean, [False]) \\\n",
    "            .build()\n",
    "    ),\n",
    "]\n",
    "\n",
    "clustering_list = [\n",
    "    (\n",
    "        kmeans, \n",
    "        ParamGridBuilder() \\\n",
    "            .addGrid(kmeans.maxIter, [20, 50])\\\n",
    "            .addGrid(scaler.withMean, [False, True]) \\\n",
    "            .build()\n",
    "    ),\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract number of features and define models for PCA\n",
    "#we will use the same classifiers, however need to define the models again because of some features adapted to the PCA\n",
    "n_features_pca = 25\n",
    "\n",
    "# Define estimators\n",
    "log_reg = LogisticRegression(standardization=False)\n",
    "\n",
    "rf = RandomForestClassifier(labelCol=\"label\", seed = SEED)\n",
    "\n",
    "mlp_layers_pca = [n_features_pca, int(n_features_pca / 2), int(n_features_pca / 4), n_classes]\n",
    "mlp_pca = MultilayerPerceptronClassifier(layers=mlp_layers_pca, blockSize=128, seed=SEED)\n",
    "\n",
    "kmeans = KMeans(k=n_classes, seed=SEED)\n",
    "\n",
    "# Create a list with each defined estimator and its grid param\n",
    "estimators_list_pca = [\n",
    "    (\n",
    "        log_reg, \n",
    "        ParamGridBuilder() \\\n",
    "            .addGrid(log_reg.maxIter, [50, 100]) \\\n",
    "            .addGrid(log_reg.tol, [1E-6]) \\\n",
    "            .addGrid(log_reg.fitIntercept, [False]) \\\n",
    "            .addGrid(scaler.withMean, [False]) \\\n",
    "            .build()\n",
    "    ),\n",
    "    (\n",
    "        rf,\n",
    "        ParamGridBuilder() \\\n",
    "            .addGrid(rf.maxDepth, [10, 15]) \\\n",
    "            .addGrid(rf.maxBins, [5, 10, 20]) \\\n",
    "            .addGrid(rf.numTrees, [10, 20]) \n",
    "            .addGrid(scaler.withMean, [False, True]) \\\n",
    "            .build()\n",
    "    ),\n",
    "    (\n",
    "        mlp_pca, \n",
    "        ParamGridBuilder() \\\n",
    "            .addGrid(mlp.maxIter, [25, 50]) \\\n",
    "            .addGrid(mlp.tol, [1E-6]) \\\n",
    "            .addGrid(scaler.withMean, [False]) \\\n",
    "            .build()\n",
    "    ),\n",
    "]\n",
    "\n",
    "clustering_list = [\n",
    "    (\n",
    "        kmeans, \n",
    "        ParamGridBuilder() \\\n",
    "            .addGrid(kmeans.maxIter, [20, 50])\\\n",
    "            .addGrid(scaler.withMean, [False, True]) \\\n",
    "            .build()\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. PCA: create the PCA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "\n",
    "pca = PCA(\n",
    "    k=n_features_pca,\n",
    "    inputCol='features',\n",
    "    outputCol='pcaFeatures'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3. Perform CV for models (without PCA step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression_4698bb555695f6efcda9\n",
      "CPU times: user 2.59 s, sys: 872 ms, total: 3.46 s\n",
      "Wall time: 11min 42s\n",
      "RandomForestClassifier_474a90300003cd62675a\n",
      "CPU times: user 18.3 s, sys: 6.84 s, total: 25.2 s\n",
      "Wall time: 1h 15min 7s\n",
      "MultilayerPerceptronClassifier_458a8220b24b620b0ee1\n",
      "CPU times: user 10.9 s, sys: 4.02 s, total: 14.9 s\n",
      "Wall time: 1h 58min 41s\n"
     ]
    }
   ],
   "source": [
    "# Save the results of the cross-validation and hyperparameter tuning in a list of models\n",
    "model_list = []\n",
    "for estimator, paramGrid in estimator_list:\n",
    "    # Set features column\n",
    "    estimator.setFeaturesCol('scaledFeatures')\n",
    "    \n",
    "    pipeline = Pipeline(stages=[scaler, estimator])\n",
    "    cvModel = CrossValidator(\n",
    "        estimator=pipeline,\n",
    "        estimatorParamMaps=paramGrid,\n",
    "        evaluator=MulticlassClassificationEvaluator(metricName='accuracy'),\n",
    "        numFolds=5\n",
    "    )\n",
    "    print(estimator) \n",
    "    %time cvModel = cvModel.fit(train_df)\n",
    "    \n",
    "    model_list.append(cvModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4. Perform CV for models (with PCA step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression_484eaf71803d0b80b0d6\n",
      "CPU times: user 3.06 s, sys: 1.1 s, total: 4.16 s\n",
      "Wall time: 7min 23s\n",
      "RandomForestClassifier_48a090634c9a1cc6150a\n",
      "CPU times: user 18.7 s, sys: 5.96 s, total: 24.6 s\n",
      "Wall time: 27min 45s\n",
      "MultilayerPerceptronClassifier_41a1a7dfc612324a502f\n",
      "CPU times: user 4.56 s, sys: 1.52 s, total: 6.08 s\n",
      "Wall time: 9min 1s\n"
     ]
    }
   ],
   "source": [
    "# Save the results of the cross-validation and hyperparameter tuning in a list of models\n",
    "model_list_pca = []\n",
    "for estimator, paramGrid in estimators_list_pca:\n",
    "    # Set features column\n",
    "    estimator.setFeaturesCol('pcaFeatures')\n",
    "    \n",
    "    pipeline = Pipeline(stages=[scaler, pca, estimator])\n",
    "    cvModel = CrossValidator(\n",
    "        estimator=pipeline,\n",
    "        estimatorParamMaps=paramGrid,\n",
    "        evaluator=MulticlassClassificationEvaluator(metricName='accuracy'),\n",
    "        numFolds=5,\n",
    "        parallelism=2\n",
    "    )\n",
    "    print(estimator)\n",
    "    %time cvModel = cvModel.fit(train_df)\n",
    "    \n",
    "    model_list_pca.append(cvModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5. Perform CV for clustering models (without PCA step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KMeans_b5816906ce6d\n",
      "Wall time: 8min 17s\n"
     ]
    }
   ],
   "source": [
    "# Save the results of the cross-validation and hyperparameter tuning in a list of models\n",
    "cluster_list = []\n",
    "for estimator, paramGrid in clustering_list:\n",
    "    # Set features column\n",
    "    estimator.setFeaturesCol('scaledFeatures')\n",
    "    \n",
    "    pipeline = Pipeline(stages=[scaler, estimator])\n",
    "    cvModel = CrossValidator(\n",
    "        estimator=pipeline,\n",
    "        estimatorParamMaps=paramGrid,\n",
    "        evaluator=ClusteringEvaluator(),\n",
    "        numFolds=5,\n",
    "        parallelism=2\n",
    "    )\n",
    "    print(estimator)\n",
    "    %time cvModel = cvModel.fit(train_df)\n",
    "    \n",
    "    cluster_list.append(cvModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6  Perform CV for clustering models (with PCA step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KMeans_b5816906ce6d\n",
      "Wall time: 6min 40s\n"
     ]
    }
   ],
   "source": [
    "# Save the results of the cross-validation and hyperparameter tuning in a list of models\n",
    "cluster_list_pca = []\n",
    "for estimator, paramGrid in clustering_list:\n",
    "    # Set features column\n",
    "    estimator.setFeaturesCol('pcaFeatures')\n",
    "    \n",
    "    pipeline = Pipeline(stages=[scaler, pca, estimator])\n",
    "    cvModel = CrossValidator(\n",
    "        estimator=pipeline,\n",
    "        estimatorParamMaps=paramGrid,\n",
    "        evaluator=ClusteringEvaluator(),\n",
    "        numFolds=5,\n",
    "        parallelism=2\n",
    "    )\n",
    "    print(estimator)\n",
    "    %time cvModel = cvModel.fit(train_df)\n",
    "    \n",
    "    cluster_list_pca.append(cvModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1. Load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please read the documentation of PySpark to learn more about the possibilities to load data files.\n",
    "# PySpark documentation: https://spark.apache.org/docs/2.0.1/api/python/pyspark.sql.html#pyspark.sql.SparkSession\n",
    "# The SparkSession object is already initialized for you.\n",
    "# The following variable contains the path to your file on your IBM Cloud Object Storage.\n",
    "test_file = cos.url('mnist_test.csv.bz2', 'filtradocollab-donotdelete-pr-dchkxqx3kk4b5a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+---+---+---+---+---+---+\n",
      "|label|1x1|1x2|1x3|1x4|1x5|1x6|1x7|\n",
      "+-----+---+---+---+---+---+---+---+\n",
      "|    7|  0|  0|  0|  0|  0|  0|  0|\n",
      "|    2|  0|  0|  0|  0|  0|  0|  0|\n",
      "|    1|  0|  0|  0|  0|  0|  0|  0|\n",
      "|    0|  0|  0|  0|  0|  0|  0|  0|\n",
      "+-----+---+---+---+---+---+---+---+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#test_file = 'mnist/mnist_test.csv.bz2'\n",
    "test_df_raw = spark.read.csv(test_file, header=True, inferSchema=True)\n",
    "test_df_raw.select(test_df_raw.columns[:8]).show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2. Update data schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "mapping = {column: 'integer' for column in test_df_raw.columns}\n",
    "\n",
    "mapping_dict = dict(mapping)\n",
    "\n",
    "exprs = [col(c).cast(mapping[c]) if c in mapping_dict else c \n",
    "         for c in test_df_raw.columns]\n",
    "\n",
    "test_df_raw = test_df_raw.select(*exprs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3. Add 'features' column as a Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|    7|(784,[202,203,204...|\n",
      "|    2|(784,[94,95,96,97...|\n",
      "|    1|(784,[128,129,130...|\n",
      "|    0|(784,[124,125,126...|\n",
      "+-----+--------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(inputCols=test_df_raw.columns[1:], outputCol='features')\n",
    "test_df = assembler.transform(test_df_raw)\n",
    "\n",
    "test_df.select(['label', 'features']).show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4. Predict using the best models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [model.transform(test_df) for model in model_list]\n",
    "predictions_pca = [model.transform(test_df) for model in model_list_pca]\n",
    "predictions_clust = [model.transform(test_df) for model in cluster_list]\n",
    "predictions_clust_pca = [model.transform(test_df) for model in cluster_list_pca]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5. Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_crosstab(model_list, predictions):\n",
    "    assert len(model_list) == len(predictions)\n",
    "    \n",
    "    for i in range(len(model_list)):\n",
    "        print('Pipeline:', model_list[i].bestModel.stages[:-1])\n",
    "        print('Model:', model_list[i].bestModel.stages[-1].__str__())\n",
    "        predictions[i].crosstab('label', 'prediction').sort('label_prediction').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Models without PCA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline: [StandardScaler_456fb8b83d10d193a4dc]\n",
      "Model: LogisticRegression_4698bb555695f6efcda9\n",
      "+----------------+---+----+---+---+---+---+---+---+---+---+\n",
      "|label_prediction|0.0| 1.0|2.0|3.0|4.0|5.0|6.0|7.0|8.0|9.0|\n",
      "+----------------+---+----+---+---+---+---+---+---+---+---+\n",
      "|               0|960|   0|  1|  2|  2|  4|  7|  1|  2|  1|\n",
      "|               1|  0|1113|  5|  2|  0|  1|  3|  2|  9|  0|\n",
      "|               2|  5|  10|927| 16|  7|  3| 12| 11| 39|  2|\n",
      "|               3|  2|   0| 17|926|  1| 22|  3|  9| 24|  6|\n",
      "|               4|  1|   1|  3|  2|915|  0| 12|  5| 10| 33|\n",
      "|               5|  8|   2|  2| 34|  8|769| 13| 11| 38|  7|\n",
      "|               6| 10|   3|  8|  1|  7| 15|910|  2|  2|  0|\n",
      "|               7|  1|   5| 24|  8|  4|  1|  0|949|  5| 31|\n",
      "|               8|  6|  11|  6| 22|  9| 25|  9| 13|865|  8|\n",
      "|               9|  8|   8|  2|  8| 25|  5|  0| 19| 11|923|\n",
      "+----------------+---+----+---+---+---+---+---+---+---+---+\n",
      "\n",
      "Pipeline: [StandardScaler_456fb8b83d10d193a4dc]\n",
      "Model: RandomForestClassificationModel (uid=RandomForestClassifier_474a90300003cd62675a) with 20 trees\n",
      "+----------------+---+----+---+---+---+---+---+---+---+---+\n",
      "|label_prediction|0.0| 1.0|2.0|3.0|4.0|5.0|6.0|7.0|8.0|9.0|\n",
      "+----------------+---+----+---+---+---+---+---+---+---+---+\n",
      "|               0|967|   0|  1|  0|  0|  2|  6|  1|  3|  0|\n",
      "|               1|  0|1119|  4|  4|  0|  1|  2|  1|  3|  1|\n",
      "|               2|  3|   1|988|  8|  7|  0|  5|  9| 10|  1|\n",
      "|               3|  1|   1| 11|960|  0| 11|  2|  9| 10|  5|\n",
      "|               4|  1|   0|  1|  1|936|  0|  7|  0|  7| 29|\n",
      "|               5|  4|   2|  0| 27|  4|826| 12|  3| 11|  3|\n",
      "|               6|  7|   4|  2|  0|  5|  1|935|  0|  4|  0|\n",
      "|               7|  2|   3| 19|  4|  3|  0|  1|979|  1| 16|\n",
      "|               8|  6|   0|  8| 11|  7|  7|  4|  5|916| 10|\n",
      "|               9|  5|   5|  0| 13| 13|  5|  1|  2|  7|958|\n",
      "+----------------+---+----+---+---+---+---+---+---+---+---+\n",
      "\n",
      "Pipeline: [StandardScaler_456fb8b83d10d193a4dc]\n",
      "Model: MultilayerPerceptronClassifier_458a8220b24b620b0ee1\n",
      "+----------------+---+----+---+---+---+---+---+---+---+---+\n",
      "|label_prediction|0.0| 1.0|2.0|3.0|4.0|5.0|6.0|7.0|8.0|9.0|\n",
      "+----------------+---+----+---+---+---+---+---+---+---+---+\n",
      "|               0|967|   0|  2|  1|  0|  5|  3|  1|  1|  0|\n",
      "|               1|  0|1116|  3|  3|  0|  2|  2|  2|  7|  0|\n",
      "|               2|  7|   0|992|  7|  3|  0|  6|  7|  9|  1|\n",
      "|               3|  0|   3| 12|958|  0| 14|  0|  7| 13|  3|\n",
      "|               4|  2|   1|  4|  1|941|  0| 11|  6|  3| 13|\n",
      "|               5|  4|   1|  0| 15|  2|846|  9|  2|  9|  4|\n",
      "|               6|  7|   3|  2|  1|  4| 10|929|  0|  2|  0|\n",
      "|               7|  1|   8| 12|  9|  1|  1|  0|980|  1| 15|\n",
      "|               8|  7|   1|  5| 10|  5| 18|  5|  7|908|  8|\n",
      "|               9|  2|   5|  2|  5| 17|  7|  1|  8|  6|956|\n",
      "+----------------+---+----+---+---+---+---+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_crosstab(model_list, predictions)\n",
    "print_crosstab(model_list_pca, predictions_pca)\n",
    "print_crosstab(cluster_list, predictions_clust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline: [StandardScaler_9b98e64ce93c]\n",
      "Model: KMeans_b5816906ce6d\n",
      "+----------------+----+---+---+---+---+---+---+---+---+\n",
      "|label_prediction|   0|  1|  2|  3|  4|  5|  6|  7|  9|\n",
      "+----------------+----+---+---+---+---+---+---+---+---+\n",
      "|               0|   1| 15|  1|480|174|  2|255| 15| 37|\n",
      "|               1|1114|  2|  0|  0|  2|  0|  3| 10|  4|\n",
      "|               2| 104|310| 10|  6|226| 14|119|  8|235|\n",
      "|               3|  59|190| 19|  1|129| 18|559| 18| 17|\n",
      "|               4|  49|  8|204| 10|  3|514|  1|165| 28|\n",
      "|               5|  54| 47| 18|  7| 46| 38|372|298| 12|\n",
      "|               6|  55| 26|  4| 65|  7|  4| 26|  5|766|\n",
      "|               7|  74|  4|134|  0|  2|617|  1|194|  2|\n",
      "|               8|  96| 44| 22|  4| 19| 58|521|200| 10|\n",
      "|               9|  31|  2|210|  3|  9|654| 19| 80|  1|\n",
      "+----------------+----+---+---+---+---+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_crosstab(cluster_list, predictions_clust)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Models with PCA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline: [StandardScaler_4200ac3d67976e896566, PCA_40c996a054db5d8e508d]\n",
      "Model: LogisticRegression_484eaf71803d0b80b0d6\n",
      "+----------------+---+----+---+---+---+---+---+---+---+---+\n",
      "|label_prediction|0.0| 1.0|2.0|3.0|4.0|5.0|6.0|7.0|8.0|9.0|\n",
      "+----------------+---+----+---+---+---+---+---+---+---+---+\n",
      "|               0|946|   0|  7|  5|  0|  8|  8|  2|  4|  0|\n",
      "|               1|  0|1108|  3|  2|  0|  1|  3|  1| 17|  0|\n",
      "|               2|  8|  11|876| 22| 19|  5| 19| 16| 42| 14|\n",
      "|               3|  4|   0| 23|894|  2| 41|  2| 15| 21|  8|\n",
      "|               4|  3|   2|  3|  0|889|  2| 15|  3| 14| 51|\n",
      "|               5| 13|   3| 12| 60| 21|707| 18|  8| 42|  8|\n",
      "|               6| 20|   3|  8|  1| 20| 12|888|  1|  5|  0|\n",
      "|               7|  5|   6| 39|  3| 13|  0|  0|918|  7| 37|\n",
      "|               8| 11|   9| 14| 37|  9| 44| 12|  7|813| 18|\n",
      "|               9|  9|  12|  9|  8| 53| 19|  1| 35|  8|855|\n",
      "+----------------+---+----+---+---+---+---+---+---+---+---+\n",
      "\n",
      "Pipeline: [StandardScaler_4200ac3d67976e896566, PCA_40c996a054db5d8e508d]\n",
      "Model: RandomForestClassificationModel (uid=RandomForestClassifier_48a090634c9a1cc6150a) with 20 trees\n",
      "+----------------+---+----+---+---+---+---+---+---+---+---+\n",
      "|label_prediction|0.0| 1.0|2.0|3.0|4.0|5.0|6.0|7.0|8.0|9.0|\n",
      "+----------------+---+----+---+---+---+---+---+---+---+---+\n",
      "|               0|955|   0|  4|  1|  1|  6|  9|  1|  3|  0|\n",
      "|               1|  0|1117|  2|  4|  0|  1|  4|  0|  6|  1|\n",
      "|               2|  8|   1|956| 16|  7|  1|  6|  9| 26|  2|\n",
      "|               3|  2|   0| 10|935|  1| 22|  1|  7| 26|  6|\n",
      "|               4|  1|   2|  6|  1|911|  5|  9|  5|  8| 34|\n",
      "|               5|  7|   1|  5| 17|  5|832|  8|  3|  5|  9|\n",
      "|               6|  6|   2|  2|  0|  6|  8|932|  0|  2|  0|\n",
      "|               7|  1|  10| 22|  5|  6|  2|  2|945|  7| 28|\n",
      "|               8|  3|   1| 13| 26| 10| 25|  5|  6|874| 11|\n",
      "|               9|  3|   8|  3| 15| 28| 12|  0| 12|  9|919|\n",
      "+----------------+---+----+---+---+---+---+---+---+---+---+\n",
      "\n",
      "Pipeline: [StandardScaler_4200ac3d67976e896566, PCA_40c996a054db5d8e508d]\n",
      "Model: MultilayerPerceptronClassifier_41a1a7dfc612324a502f\n",
      "+----------------+---+----+---+---+---+---+---+---+---+---+\n",
      "|label_prediction|0.0| 1.0|2.0|3.0|4.0|5.0|6.0|7.0|8.0|9.0|\n",
      "+----------------+---+----+---+---+---+---+---+---+---+---+\n",
      "|               0|918|   0| 25|  6|  0| 14| 10|  3|  4|  0|\n",
      "|               1|  0|1097|  4|  8|  1|  7|  5|  0| 12|  1|\n",
      "|               2| 35|   9|709| 62| 44|  1| 57| 29| 67| 19|\n",
      "|               3| 21|   2| 10|786|  4| 90|  9| 43| 41|  4|\n",
      "|               4|  3|   6|  9|  0|791|  1|  7|  7| 16|142|\n",
      "|               5| 63|  18| 24|114| 29|575| 18| 11| 37|  3|\n",
      "|               6| 22|   9|136|  2|  6|  4|768|  1|  8|  2|\n",
      "|               7|  4|   6|  6|  8|  3|  5|  0|898| 32| 66|\n",
      "|               8|  9|  18| 35| 48|  4| 20|  4| 19|793| 24|\n",
      "|               9| 11|   6|  4| 16|197|  8|  3| 95| 31|638|\n",
      "+----------------+---+----+---+---+---+---+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_crosstab(model_list_pca, predictions_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6. Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def print_metrics(cvModel):\n",
    "    model = cvModel.bestModel.stages[-1]\n",
    "    summary = model.summary\n",
    "    \n",
    "    print('Pipeline:', cvModel.bestModel.stages[:-1])\n",
    "    print('Model:', model.__str__())\n",
    "    print('Accuracy:', summary.accuracy)\n",
    "    \n",
    "    f_measure = summary.fMeasureByLabel()\n",
    "    \n",
    "    metrics_df = pd.DataFrame(\n",
    "        {\n",
    "            'Label': range(len(f_measure)),\n",
    "            'F-measure': f_measure,\n",
    "            'TPR (Recall)': summary.truePositiveRateByLabel,\n",
    "            'FPR (1 - Specificity)': summary.falsePositiveRateByLabel,\n",
    "            'Precision': summary.precisionByLabel\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    print(metrics_df.round(2).to_string(index=False))\n",
    "    \n",
    "    print()\n",
    "\n",
    "def print_custom_metrics(cvModel, predictions):\n",
    "    model = cvModel.bestModel.stages[-1]\n",
    "    \n",
    "    print('Pipeline:', cvModel.bestModel.stages[:-1])\n",
    "    print('Model:', model.__str__())\n",
    "    \n",
    "    y_true = predictions.select('label').toPandas().values.flatten()\n",
    "    y_pred = predictions.select('prediction').toPandas().values.flatten()\n",
    "    \n",
    "    print(metrics.classification_report(y_true, y_pred))\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Without PCA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline: [StandardScaler_456fb8b83d10d193a4dc]\n",
      "Model: LogisticRegression_4698bb555695f6efcda9\n",
      "Accuracy: 0.9352333333333334\n",
      "F-measure  FPR (1 - Specificity)  Label  Precision  TPR (Recall)\n",
      "     0.97                   0.00      0       0.97          0.98\n",
      "     0.97                   0.00      1       0.96          0.98\n",
      "     0.93                   0.01      2       0.94          0.92\n",
      "     0.92                   0.01      3       0.92          0.91\n",
      "     0.94                   0.01      4       0.94          0.94\n",
      "     0.90                   0.01      5       0.92          0.89\n",
      "     0.96                   0.01      6       0.95          0.97\n",
      "     0.95                   0.01      7       0.95          0.94\n",
      "     0.90                   0.01      8       0.89          0.90\n",
      "     0.92                   0.01      9       0.91          0.92\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression metrics\n",
    "log_results = []\n",
    "log_results.append(model_list[0])\n",
    "tmp = [print_metrics(model) for model in log_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline: [StandardScaler_456fb8b83d10d193a4dc]\n",
      "Model: RandomForestClassificationModel (uid=RandomForestClassifier_474a90300003cd62675a) with 20 trees\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.99      0.98       980\n",
      "          1       0.99      0.99      0.99      1135\n",
      "          2       0.96      0.96      0.96      1032\n",
      "          3       0.93      0.95      0.94      1010\n",
      "          4       0.96      0.95      0.96       982\n",
      "          5       0.97      0.93      0.95       892\n",
      "          6       0.96      0.98      0.97       958\n",
      "          7       0.97      0.95      0.96      1028\n",
      "          8       0.94      0.94      0.94       974\n",
      "          9       0.94      0.95      0.94      1009\n",
      "\n",
      "avg / total       0.96      0.96      0.96     10000\n",
      "\n",
      "\n",
      "Pipeline: [StandardScaler_456fb8b83d10d193a4dc]\n",
      "Model: MultilayerPerceptronClassifier_458a8220b24b620b0ee1\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.99      0.98       980\n",
      "          1       0.98      0.98      0.98      1135\n",
      "          2       0.96      0.96      0.96      1032\n",
      "          3       0.95      0.95      0.95      1010\n",
      "          4       0.97      0.96      0.96       982\n",
      "          5       0.94      0.95      0.94       892\n",
      "          6       0.96      0.97      0.97       958\n",
      "          7       0.96      0.95      0.96      1028\n",
      "          8       0.95      0.93      0.94       974\n",
      "          9       0.96      0.95      0.95      1009\n",
      "\n",
      "avg / total       0.96      0.96      0.96     10000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_custom_metrics(model_list[1], predictions[1])\n",
    "print_custom_metrics(model_list[2], predictions[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**With PCA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline: [StandardScaler_4200ac3d67976e896566, PCA_40c996a054db5d8e508d]\n",
      "Model: LogisticRegression_484eaf71803d0b80b0d6\n",
      "Accuracy: 0.8833833333333333\n",
      "F-measure  FPR (1 - Specificity)  Label  Precision  TPR (Recall)\n",
      "     0.94                   0.01      0       0.93          0.95\n",
      "     0.95                   0.01      1       0.94          0.97\n",
      "     0.86                   0.01      2       0.88          0.85\n",
      "     0.86                   0.02      3       0.86          0.85\n",
      "     0.89                   0.01      4       0.88          0.91\n",
      "     0.80                   0.02      5       0.82          0.78\n",
      "     0.92                   0.01      6       0.91          0.93\n",
      "     0.91                   0.01      7       0.91          0.90\n",
      "     0.83                   0.02      8       0.83          0.83\n",
      "     0.85                   0.02      9       0.85          0.85\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression metrics\n",
    "log_pca_results = []\n",
    "log_pca_results.append(model_list_pca[0])\n",
    "tmp = [print_metrics(model) for model in log_pca_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline: [StandardScaler_4200ac3d67976e896566, PCA_40c996a054db5d8e508d]\n",
      "Model: RandomForestClassificationModel (uid=RandomForestClassifier_48a090634c9a1cc6150a) with 20 trees\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.97      0.97       980\n",
      "          1       0.98      0.98      0.98      1135\n",
      "          2       0.93      0.93      0.93      1032\n",
      "          3       0.92      0.93      0.92      1010\n",
      "          4       0.93      0.93      0.93       982\n",
      "          5       0.91      0.93      0.92       892\n",
      "          6       0.95      0.97      0.96       958\n",
      "          7       0.96      0.92      0.94      1028\n",
      "          8       0.90      0.90      0.90       974\n",
      "          9       0.91      0.91      0.91      1009\n",
      "\n",
      "avg / total       0.94      0.94      0.94     10000\n",
      "\n",
      "\n",
      "Pipeline: [StandardScaler_4200ac3d67976e896566, PCA_40c996a054db5d8e508d]\n",
      "Model: MultilayerPerceptronClassifier_41a1a7dfc612324a502f\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.85      0.94      0.89       980\n",
      "          1       0.94      0.97      0.95      1135\n",
      "          2       0.74      0.69      0.71      1032\n",
      "          3       0.75      0.78      0.76      1010\n",
      "          4       0.73      0.81      0.77       982\n",
      "          5       0.79      0.64      0.71       892\n",
      "          6       0.87      0.80      0.84       958\n",
      "          7       0.81      0.87      0.84      1028\n",
      "          8       0.76      0.81      0.79       974\n",
      "          9       0.71      0.63      0.67      1009\n",
      "\n",
      "avg / total       0.80      0.80      0.80     10000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_custom_metrics(model_list_pca[1], predictions_pca[1])\n",
    "print_custom_metrics(model_list_pca[2], predictions_pca[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tras la implementación de los diferentes algoritmos de clasificación, podemos observar como estos tienen un mejor rendimiento cuando no se emplea el uso de la PCA. \n",
    "\n",
    "Entre todos los métodos de clasificación implementados, Multilayer Perceptron, Random Forest y Logistic Regression, se obtuvo el mejor rendimiento con el Random Forest y el MLP sin PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation of clustering methods\n",
    "import pandas as pd\n",
    "\n",
    "def print_cluster_metrics(cvModel):\n",
    "    model = cvModel.bestModel.stages[-1]\n",
    "    summary = model.summary\n",
    "    \n",
    "    print('Pipeline:', cvModel.bestModel.stages[:-1])\n",
    "    print('Model:', model.__str__())\n",
    "    \n",
    "    metrics_df = pd.DataFrame(\n",
    "        {\n",
    "            'Label': range(len(summary.clusterSizes)),\n",
    "            'clusterSizes': summary.clusterSizes,\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    print(metrics_df.round(2).to_string(index=False))\n",
    "    \n",
    "    print('trainingCost:', round(summary.trainingCost, 2))\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline: [StandardScaler_9b98e64ce93c]\n",
      "Model: KMeans_b5816906ce6d\n",
      " Label  clusterSizes\n",
      "     0         10279\n",
      "     1          4140\n",
      "     2          3916\n",
      "     3          3361\n",
      "     4          4233\n",
      "     5         10889\n",
      "     6         10033\n",
      "     7          6457\n",
      "     8             4\n",
      "     9          6688\n",
      "trainingCost: 36684486.11\n",
      "\n",
      "Pipeline: [StandardScaler_9b98e64ce93c, PCA_f1334576f97f]\n",
      "Model: KMeans_b5816906ce6d\n",
      " Label  clusterSizes\n",
      "     0          5442\n",
      "     1          4999\n",
      "     2          6861\n",
      "     3          5404\n",
      "     4          4587\n",
      "     5          5226\n",
      "     6          7263\n",
      "     7          7633\n",
      "     8          5689\n",
      "     9          6896\n",
      "trainingCost: 90086714778.98\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp = [print_cluster_metrics(model) for model in cluster_list + cluster_list_pca]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def print_cluster_metrics(cvModel, predictions):\n",
    "    model = cvModel.bestModel.stages[-1]\n",
    "    summary = model.summary\n",
    "    \n",
    "    print('Pipeline:', cvModel.bestModel.stages[:-1])\n",
    "    print('Model:', model.__str__())\n",
    "    \n",
    "    y_true = predictions.select('label').toPandas().values.flatten()\n",
    "    y_pred = predictions.select('prediction').toPandas().values.flatten()\n",
    "    \n",
    "    metrics_df = pd.DataFrame(\n",
    "        {\n",
    "            'Label': range(len(summary.clusterSizes)),\n",
    "            'clusterSizes': summary.clusterSizes,\n",
    "            'F-measure': metrics.f1_score(y_true, y_pred, average='micro'),\n",
    "            'TPR (Recall)': metrics.recall_score(y_true, y_pred, average='micro'),\n",
    "            'Precision': metrics.precision_score(y_true, y_pred, average='micro')\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    print(metrics_df.round(2).to_string(index=False))\n",
    "    \n",
    "    print('\\ntrainingCost:', round(summary.trainingCost, 2))\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clustering without PCA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline: [StandardScaler_9b98e64ce93c]\n",
      "Model: KMeans_b5816906ce6d\n",
      " Label  clusterSizes  F-measure  TPR (Recall)  Precision\n",
      "     0         10279       0.03          0.03       0.03\n",
      "     1          4140       0.03          0.03       0.03\n",
      "     2          3916       0.03          0.03       0.03\n",
      "     3          3361       0.03          0.03       0.03\n",
      "     4          4233       0.03          0.03       0.03\n",
      "     5         10889       0.03          0.03       0.03\n",
      "     6         10033       0.03          0.03       0.03\n",
      "     7          6457       0.03          0.03       0.03\n",
      "     8             4       0.03          0.03       0.03\n",
      "     9          6688       0.03          0.03       0.03\n",
      "\n",
      "trainingCost: 36684486.11\n",
      "\n"
     ]
    }
   ],
   "source": [
    "assert len(cluster_list) == len(predictions_clust)\n",
    "\n",
    "for i in range(len(cluster_list)):\n",
    "    print_cluster_metrics(cluster_list[i], predictions_clust[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clustering with PCA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline: [StandardScaler_9b98e64ce93c, PCA_f1334576f97f]\n",
      "Model: KMeans_b5816906ce6d\n",
      " Label  clusterSizes  F-measure  TPR (Recall)  Precision\n",
      "     0          5442       0.04          0.04       0.04\n",
      "     1          4999       0.04          0.04       0.04\n",
      "     2          6861       0.04          0.04       0.04\n",
      "     3          5404       0.04          0.04       0.04\n",
      "     4          4587       0.04          0.04       0.04\n",
      "     5          5226       0.04          0.04       0.04\n",
      "     6          7263       0.04          0.04       0.04\n",
      "     7          7633       0.04          0.04       0.04\n",
      "     8          5689       0.04          0.04       0.04\n",
      "     9          6896       0.04          0.04       0.04\n",
      "\n",
      "trainingCost: 90086714778.98\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# predicting pca\n",
    "assert len(cluster_list_pca) == len(predictions_clust_pca)\n",
    "\n",
    "for i in range(len(cluster_list_pca)):\n",
    "    print_cluster_metrics(cluster_list_pca[i], predictions_clust_pca[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el caso del algoritmo de clustering, k-means, los resultados de rendimiento en comparación con los resultados obtenidos con los métodos de clasificación de aprendizaje supervisado fueron significativamente peores. \n",
    "\n",
    "Con respeto al uso del PCA, se obtuvieron resultados de rendimiento ligeramente más favorables con el uso del PCA. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
